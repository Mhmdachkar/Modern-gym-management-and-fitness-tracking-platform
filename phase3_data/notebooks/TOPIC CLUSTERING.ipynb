{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a763d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TOPIC CLUSTERING ANALYSIS - GYM MANAGEMENT SYSTEM\n",
    "# Phase 3 - Task 9: Group user queries/feedback into thematic clusters\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SETUP AND IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "# Install required packages (run only once in Colab)\n",
    "!pip install wordcloud scikit-learn plotly\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Text processing and NLP\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Clustering algorithms\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Dimensionality reduction and visualization\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "\n",
    "# Word cloud and text visualization\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "# Metrics and evaluation\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')  # Added missing resource\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA LOADING AND INITIAL EXPLORATION\n",
    "# =============================================================================\n",
    "\n",
    "# Load the feedback data\n",
    "# Replace 'feedback_data.csv' with your actual file path\n",
    "df = pd.read_csv('feedback_data.csv')\n",
    "\n",
    "print(\"ðŸ” DATASET OVERVIEW\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nðŸ“Š FEEDBACK DISTRIBUTION\")\n",
    "print(\"=\"*50)\n",
    "print(\"Feedback types:\")\n",
    "print(df['feedback_type'].value_counts())\n",
    "print(\"\\nRating distribution:\")\n",
    "print(df['rating'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nðŸ“ TEXT DATA ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "df['text_length'] = df['feedback_text'].str.len()\n",
    "df['word_count'] = df['feedback_text'].str.split().str.len()\n",
    "\n",
    "print(f\"Average text length: {df['text_length'].mean():.1f} characters\")\n",
    "print(f\"Average word count: {df['word_count'].mean():.1f} words\")\n",
    "print(f\"Minimum words: {df['word_count'].min()}\")\n",
    "print(f\"Maximum words: {df['word_count'].max()}\")\n",
    "\n",
    "# Sample feedback texts\n",
    "print(\"\\nðŸ“‹ SAMPLE FEEDBACK TEXTS\")\n",
    "print(\"=\"*50)\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i+1}: {df['feedback_text'].iloc[i]}\")\n",
    "    print(\"-\"*40)\n",
    "\n",
    "# =============================================================================\n",
    "# 3. TEXT PREPROCESSING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        # Add gym-specific stop words\n",
    "        self.stop_words.update(['gym', 'class', 'workout', 'exercise', 'session', 'coach', 'instructor'])\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    \n",
    "    def tokenize_and_lemmatize(self, text):\n",
    "        \"\"\"Tokenize and lemmatize text\"\"\"\n",
    "        try:\n",
    "            tokens = word_tokenize(text)\n",
    "        except LookupError:\n",
    "            # Fallback to simple split if NLTK resources are missing\n",
    "            tokens = text.split()\n",
    "        \n",
    "        # Remove stop words and lemmatize\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens \n",
    "                 if token not in self.stop_words and len(token) > 2]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "        text = self.clean_text(text)\n",
    "        text = self.tokenize_and_lemmatize(text)\n",
    "        return text\n",
    "\n",
    "# Initialize preprocessor and clean the data\n",
    "print(\"ðŸ§¹ PREPROCESSING TEXT DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "df['cleaned_text'] = df['feedback_text'].apply(preprocessor.preprocess)\n",
    "\n",
    "# Show before and after examples\n",
    "print(\"BEFORE AND AFTER PREPROCESSING:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nOriginal: {df['feedback_text'].iloc[i]}\")\n",
    "    print(f\"Cleaned:  {df['cleaned_text'].iloc[i]}\")\n",
    "    print(\"-\"*40)\n",
    "\n",
    "# Remove empty or very short cleaned texts\n",
    "df_clean = df[df['cleaned_text'].str.len() > 10].copy()\n",
    "print(f\"\\nâœ… Preprocessing complete! {len(df_clean)} valid feedback entries ready for clustering.\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. TEXT VECTORIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ðŸ”¤ TEXT VECTORIZATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=100,  # Top 100 features\n",
    "    min_df=2,          # Minimum document frequency\n",
    "    max_df=0.8,        # Maximum document frequency\n",
    "    ngram_range=(1, 2) # Unigrams and bigrams\n",
    ")\n",
    "\n",
    "# Fit and transform the cleaned text\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df_clean['cleaned_text'])\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {X_tfidf.shape}\")\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "\n",
    "# Show top features\n",
    "feature_scores = X_tfidf.sum(axis=0).A1\n",
    "top_features_idx = feature_scores.argsort()[-20:][::-1]\n",
    "print(\"\\nTop 20 features by TF-IDF score:\")\n",
    "for idx in top_features_idx:\n",
    "    print(f\"  {feature_names[idx]}: {feature_scores[idx]:.2f}\")\n",
    "\n",
    "# Count Vectorization (for LDA)\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_features=50,\n",
    "    min_df=2,\n",
    "    max_df=0.8,\n",
    "    ngram_range=(1, 1)  # Only unigrams for LDA\n",
    ")\n",
    "\n",
    "X_count = count_vectorizer.fit_transform(df_clean['cleaned_text'])\n",
    "count_feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"\\nCount matrix shape: {X_count.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 5. OPTIMAL NUMBER OF CLUSTERS ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ðŸ“Š DETERMINING OPTIMAL NUMBER OF CLUSTERS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Elbow method for K-means\n",
    "def find_optimal_clusters(X, max_k=10):\n",
    "    sse = []\n",
    "    silhouette_scores = []\n",
    "    calinski_scores = []\n",
    "    \n",
    "    K_range = range(2, max_k+1)\n",
    "    \n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X)\n",
    "        \n",
    "        sse.append(kmeans.inertia_)\n",
    "        \n",
    "        # Calculate silhouette score\n",
    "        sil_score = silhouette_score(X.toarray(), kmeans.labels_)\n",
    "        silhouette_scores.append(sil_score)\n",
    "        \n",
    "        # Calculate Calinski-Harabasz score\n",
    "        ch_score = calinski_harabasz_score(X.toarray(), kmeans.labels_)\n",
    "        calinski_scores.append(ch_score)\n",
    "    \n",
    "    return K_range, sse, silhouette_scores, calinski_scores\n",
    "\n",
    "K_range, sse, silhouette_scores, calinski_scores = find_optimal_clusters(X_tfidf)\n",
    "\n",
    "# Plot the results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Elbow curve\n",
    "axes[0].plot(K_range, sse, 'bo-')\n",
    "axes[0].set_xlabel('Number of Clusters (k)')\n",
    "axes[0].set_ylabel('Sum of Squared Errors (SSE)')\n",
    "axes[0].set_title('Elbow Method for Optimal k')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Silhouette scores\n",
    "axes[1].plot(K_range, silhouette_scores, 'ro-')\n",
    "axes[1].set_xlabel('Number of Clusters (k)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Analysis')\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Calinski-Harabasz scores\n",
    "axes[2].plot(K_range, calinski_scores, 'go-')\n",
    "axes[2].set_xlabel('Number of Clusters (k)')\n",
    "axes[2].set_ylabel('Calinski-Harabasz Score')\n",
    "axes[2].set_title('Calinski-Harabasz Analysis')\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal k based on silhouette score\n",
    "optimal_k = K_range[np.argmax(silhouette_scores)]\n",
    "print(f\"\\nðŸŽ¯ Recommended number of clusters based on silhouette score: {optimal_k}\")\n",
    "print(f\"Best silhouette score: {max(silhouette_scores):.3f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 6. CLUSTERING ALGORITHMS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ðŸ” APPLYING CLUSTERING ALGORITHMS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 6.1 K-Means Clustering\n",
    "print(\"1. K-Means Clustering\")\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "df_clean['kmeans_cluster'] = kmeans.fit_predict(X_tfidf)\n",
    "\n",
    "print(f\"K-Means cluster distribution:\")\n",
    "print(df_clean['kmeans_cluster'].value_counts().sort_index())\n",
    "\n",
    "# 6.2 DBSCAN Clustering\n",
    "print(\"\\n2. DBSCAN Clustering\")\n",
    "# Try different epsilon values\n",
    "eps_values = [0.3, 0.5, 0.7, 0.9]\n",
    "best_eps = 0.5\n",
    "best_n_clusters = 0\n",
    "\n",
    "for eps in eps_values:\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=3)\n",
    "    labels = dbscan.fit_predict(X_tfidf.toarray())\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    if n_clusters > best_n_clusters and n_clusters < 8:\n",
    "        best_eps = eps\n",
    "        best_n_clusters = n_clusters\n",
    "\n",
    "dbscan = DBSCAN(eps=best_eps, min_samples=3)\n",
    "df_clean['dbscan_cluster'] = dbscan.fit_predict(X_tfidf.toarray())\n",
    "\n",
    "print(f\"DBSCAN cluster distribution (eps={best_eps}):\")\n",
    "print(df_clean['dbscan_cluster'].value_counts().sort_index())\n",
    "\n",
    "# 6.3 Hierarchical Clustering\n",
    "print(\"\\n3. Hierarchical Clustering\")\n",
    "hierarchical = AgglomerativeClustering(n_clusters=optimal_k)\n",
    "df_clean['hierarchical_cluster'] = hierarchical.fit_predict(X_tfidf.toarray())\n",
    "\n",
    "print(f\"Hierarchical cluster distribution:\")\n",
    "print(df_clean['hierarchical_cluster'].value_counts().sort_index())\n",
    "\n",
    "# 6.4 Topic Modeling with LDA\n",
    "print(\"\\n4. Latent Dirichlet Allocation (LDA)\")\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=optimal_k,\n",
    "    random_state=42,\n",
    "    max_iter=100\n",
    ")\n",
    "lda.fit(X_count)\n",
    "\n",
    "# Get topic assignments\n",
    "topic_probs = lda.transform(X_count)\n",
    "df_clean['lda_topic'] = topic_probs.argmax(axis=1)\n",
    "\n",
    "print(f\"LDA topic distribution:\")\n",
    "print(df_clean['lda_topic'].value_counts().sort_index())\n",
    "\n",
    "# =============================================================================\n",
    "# 7. CLUSTER ANALYSIS AND INTERPRETATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ðŸ”¬ CLUSTER ANALYSIS AND INTERPRETATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def analyze_clusters(df, cluster_col, vectorizer, X, method_name):\n",
    "    \"\"\"Analyze and interpret clusters\"\"\"\n",
    "    print(f\"\\n{method_name.upper()} CLUSTER ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    for cluster_id in sorted(df[cluster_col].unique()):\n",
    "        if cluster_id == -1:  # Skip noise points in DBSCAN\n",
    "            continue\n",
    "            \n",
    "        cluster_mask = df[cluster_col] == cluster_id\n",
    "        cluster_docs = df[cluster_mask]\n",
    "        \n",
    "        print(f\"\\nðŸ“ CLUSTER {cluster_id} ({len(cluster_docs)} documents)\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Average rating in this cluster\n",
    "        avg_rating = cluster_docs['rating'].mean()\n",
    "        print(f\"Average Rating: {avg_rating:.2f}\")\n",
    "        \n",
    "        # Feedback types in this cluster\n",
    "        feedback_types = cluster_docs['feedback_type'].value_counts()\n",
    "        print(f\"Feedback Types: {dict(feedback_types)}\")\n",
    "        \n",
    "        # Top terms for this cluster\n",
    "        cluster_indices = cluster_docs.index\n",
    "        if hasattr(X, 'toarray'):\n",
    "            cluster_tfidf = X[cluster_indices].toarray().mean(axis=0)\n",
    "        else:\n",
    "            cluster_tfidf = X[cluster_indices].mean(axis=0)\n",
    "        \n",
    "        top_indices = cluster_tfidf.argsort()[-10:][::-1]\n",
    "        top_terms = [feature_names[i] for i in top_indices]\n",
    "        top_scores = [cluster_tfidf[i] for i in top_indices]\n",
    "        \n",
    "        print(\"Top Terms:\")\n",
    "        for term, score in zip(top_terms, top_scores):\n",
    "            print(f\"  â€¢ {term}: {score:.3f}\")\n",
    "        \n",
    "        # Sample feedback from this cluster\n",
    "        print(\"\\nSample Feedback:\")\n",
    "        sample_feedback = cluster_docs['feedback_text'].head(2)\n",
    "        for i, feedback in enumerate(sample_feedback, 1):\n",
    "            print(f\"  {i}. {feedback[:100]}...\")\n",
    "\n",
    "# Analyze K-Means clusters\n",
    "analyze_clusters(df_clean, 'kmeans_cluster', tfidf_vectorizer, X_tfidf, \"K-Means\")\n",
    "\n",
    "# =============================================================================\n",
    "# 8. LDA TOPIC INTERPRETATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nðŸŽ¯ LDA TOPIC INTERPRETATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words=10):\n",
    "    \"\"\"Display topics from LDA model\"\"\"\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words_idx = topic.argsort()[-no_top_words:][::-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        top_weights = [topic[i] for i in top_words_idx]\n",
    "        \n",
    "        print(f\"\\nðŸ“‹ TOPIC {topic_idx}\")\n",
    "        print(\"-\" * 30)\n",
    "        for word, weight in zip(top_words, top_weights):\n",
    "            print(f\"  {word}: {weight:.3f}\")\n",
    "        \n",
    "        topics.append({\n",
    "            'topic_id': topic_idx,\n",
    "            'words': top_words,\n",
    "            'weights': top_weights\n",
    "        })\n",
    "    \n",
    "    return topics\n",
    "\n",
    "topics = display_topics(lda, count_feature_names, no_top_words=8)\n",
    "\n",
    "# Interpret topics based on top words\n",
    "topic_labels = {}\n",
    "print(\"\\nðŸ·ï¸ TOPIC LABELS (Interpretations)\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "for i, topic in enumerate(topics):\n",
    "    words = topic['words'][:5]  # Top 5 words\n",
    "    \n",
    "    # Simple heuristic labeling based on common words\n",
    "    if any(word in ['equipment', 'machine', 'facility'] for word in words):\n",
    "        label = \"Equipment/Facility Issues\"\n",
    "    elif any(word in ['coach', 'instructor', 'teaching'] for word in words):\n",
    "        label = \"Coach/Instruction Quality\"\n",
    "    elif any(word in ['challenging', 'hard', 'intense', 'difficult'] for word in words):\n",
    "        label = \"Workout Intensity\"\n",
    "    elif any(word in ['music', 'energy', 'fun', 'amazing'] for word in words):\n",
    "        label = \"Class Atmosphere\"\n",
    "    elif any(word in ['beginner', 'advanced', 'modification'] for word in words):\n",
    "        label = \"Difficulty Level\"\n",
    "    else:\n",
    "        label = f\"General Feedback {i+1}\"\n",
    "    \n",
    "    topic_labels[i] = label\n",
    "    print(f\"Topic {i}: {label}\")\n",
    "\n",
    "df_clean['topic_label'] = df_clean['lda_topic'].map(topic_labels)\n",
    "\n",
    "# =============================================================================\n",
    "# 9. VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ðŸ“Š CREATING VISUALIZATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 9.1 Cluster Distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# K-Means distribution\n",
    "df_clean['kmeans_cluster'].value_counts().sort_index().plot(kind='bar', ax=axes[0,0])\n",
    "axes[0,0].set_title('K-Means Cluster Distribution')\n",
    "axes[0,0].set_xlabel('Cluster')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "\n",
    "# DBSCAN distribution\n",
    "df_clean['dbscan_cluster'].value_counts().sort_index().plot(kind='bar', ax=axes[0,1])\n",
    "axes[0,1].set_title('DBSCAN Cluster Distribution')\n",
    "axes[0,1].set_xlabel('Cluster')\n",
    "axes[0,1].set_ylabel('Count')\n",
    "\n",
    "# LDA topic distribution\n",
    "df_clean['lda_topic'].value_counts().sort_index().plot(kind='bar', ax=axes[1,0])\n",
    "axes[1,0].set_title('LDA Topic Distribution')\n",
    "axes[1,0].set_xlabel('Topic')\n",
    "axes[1,0].set_ylabel('Count')\n",
    "\n",
    "# Rating distribution by cluster\n",
    "df_clean.groupby('kmeans_cluster')['rating'].mean().plot(kind='bar', ax=axes[1,1])\n",
    "axes[1,1].set_title('Average Rating by K-Means Cluster')\n",
    "axes[1,1].set_xlabel('Cluster')\n",
    "axes[1,1].set_ylabel('Average Rating')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 9.2 Word Clouds for each cluster\n",
    "print(\"\\nâ˜ï¸ Generating Word Clouds\")\n",
    "\n",
    "def create_wordcloud(text_list, title):\n",
    "    \"\"\"Create word cloud from list of texts\"\"\"\n",
    "    text = ' '.join(text_list)\n",
    "    if len(text.strip()) > 0:\n",
    "        wordcloud = WordCloud(\n",
    "            width=800, \n",
    "            height=400, \n",
    "            background_color='white',\n",
    "            colormap='viridis'\n",
    "        ).generate(text)\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(title, fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create word clouds for top K-means clusters\n",
    "top_clusters = df_clean['kmeans_cluster'].value_counts().head(3).index\n",
    "\n",
    "for cluster_id in top_clusters:\n",
    "    cluster_texts = df_clean[df_clean['kmeans_cluster'] == cluster_id]['cleaned_text'].tolist()\n",
    "    create_wordcloud(cluster_texts, f'Word Cloud - K-Means Cluster {cluster_id}')\n",
    "\n",
    "# 9.3 2D Visualization of Clusters using t-SNE\n",
    "print(\"\\nðŸ—ºï¸ Creating 2D Cluster Visualization\")\n",
    "\n",
    "# Reduce dimensions for visualization\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(df_clean)-1))\n",
    "X_tsne = tsne.fit_transform(X_tfidf.toarray())\n",
    "\n",
    "# Create interactive plot with Plotly\n",
    "fig = px.scatter(\n",
    "    x=X_tsne[:, 0], \n",
    "    y=X_tsne[:, 1],\n",
    "    color=df_clean['kmeans_cluster'].astype(str),\n",
    "    title='t-SNE Visualization of Feedback Clusters',\n",
    "    labels={'x': 't-SNE 1', 'y': 't-SNE 2', 'color': 'Cluster'},\n",
    "    hover_data=[df_clean['rating'], df_clean['feedback_type']]\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# 9.4 Topic Distribution Heatmap\n",
    "print(\"\\nðŸ”¥ Creating Topic-Feedback Type Heatmap\")\n",
    "\n",
    "# Create crosstab for heatmap\n",
    "crosstab = pd.crosstab(df_clean['feedback_type'], df_clean['lda_topic'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(crosstab, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Feedback Type vs LDA Topic Distribution')\n",
    "plt.xlabel('LDA Topic')\n",
    "plt.ylabel('Feedback Type')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 10. RESULTS SUMMARY AND EXPORT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ðŸ“‹ GENERATING RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create comprehensive results summary\n",
    "results_summary = {\n",
    "    'total_feedback': len(df_clean),\n",
    "    'optimal_clusters': optimal_k,\n",
    "    'clustering_methods': ['K-Means', 'DBSCAN', 'Hierarchical', 'LDA'],\n",
    "    'best_silhouette_score': max(silhouette_scores),\n",
    "    'topic_labels': topic_labels\n",
    "}\n",
    "\n",
    "print(\"ðŸŽ¯ CLUSTERING RESULTS SUMMARY\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Total feedback analyzed: {results_summary['total_feedback']}\")\n",
    "print(f\"Optimal number of clusters: {results_summary['optimal_clusters']}\")\n",
    "print(f\"Best silhouette score: {results_summary['best_silhouette_score']:.3f}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š CLUSTER DISTRIBUTION:\")\n",
    "for method in ['kmeans_cluster', 'dbscan_cluster', 'lda_topic']:\n",
    "    print(f\"\\n{method.upper()}:\")\n",
    "    distribution = df_clean[method].value_counts().sort_index()\n",
    "    for cluster, count in distribution.items():\n",
    "        print(f\"  Cluster {cluster}: {count} feedback entries\")\n",
    "\n",
    "print(f\"\\nðŸ·ï¸ IDENTIFIED TOPICS:\")\n",
    "for topic_id, label in topic_labels.items():\n",
    "    count = (df_clean['lda_topic'] == topic_id).sum()\n",
    "    avg_rating = df_clean[df_clean['lda_topic'] == topic_id]['rating'].mean()\n",
    "    print(f\"  Topic {topic_id} - {label}: {count} entries (avg rating: {avg_rating:.2f})\")\n",
    "\n",
    "# Export results to CSV\n",
    "export_df = df_clean[[\n",
    "    'member_id', 'feedback_type', 'rating', 'feedback_text', \n",
    "    'cleaned_text', 'kmeans_cluster', 'dbscan_cluster', \n",
    "    'lda_topic', 'topic_label', 'sentiment_score'\n",
    "]].copy()\n",
    "\n",
    "export_df.to_csv('feedback_clustering_results.csv', index=False)\n",
    "\n",
    "# Export cluster summaries\n",
    "cluster_summary = []\n",
    "for cluster_id in sorted(df_clean['kmeans_cluster'].unique()):\n",
    "    cluster_data = df_clean[df_clean['kmeans_cluster'] == cluster_id]\n",
    "    \n",
    "    # Get top terms\n",
    "    cluster_indices = cluster_data.index\n",
    "    cluster_tfidf = X_tfidf[cluster_indices].toarray().mean(axis=0)\n",
    "    top_indices = cluster_tfidf.argsort()[-5:][::-1]\n",
    "    top_terms = [feature_names[i] for i in top_indices]\n",
    "    \n",
    "    cluster_summary.append({\n",
    "        'cluster_id': cluster_id,\n",
    "        'size': len(cluster_data),\n",
    "        'avg_rating': cluster_data['rating'].mean(),\n",
    "        'top_terms': ', '.join(top_terms),\n",
    "        'dominant_feedback_type': cluster_data['feedback_type'].mode().iloc[0],\n",
    "        'sample_feedback': cluster_data['feedback_text'].iloc[0][:200] + \"...\"\n",
    "    })\n",
    "\n",
    "cluster_summary_df = pd.DataFrame(cluster_summary)\n",
    "cluster_summary_df.to_csv('cluster_summary.csv', index=False)\n",
    "\n",
    "print(f\"\\nâœ… EXPORT COMPLETE!\")\n",
    "print(f\"ðŸ“ Files created:\")\n",
    "print(f\"  â€¢ feedback_clustering_results.csv - Detailed results with cluster assignments\")\n",
    "print(f\"  â€¢ cluster_summary.csv - Cluster summaries and characteristics\")\n",
    "\n",
    "# =============================================================================\n",
    "# 11. BUSINESS INSIGHTS AND RECOMMENDATIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nðŸ’¡ BUSINESS INSIGHTS AND RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"ðŸ” KEY FINDINGS:\")\n",
    "\n",
    "# Analyze ratings by cluster\n",
    "print(\"\\n1. RATING ANALYSIS BY CLUSTER:\")\n",
    "rating_by_cluster = df_clean.groupby('kmeans_cluster')['rating'].agg(['mean', 'count']).round(2)\n",
    "for cluster_id, row in rating_by_cluster.iterrows():\n",
    "    status = \"HIGH\" if row['mean'] >= 4 else \"MEDIUM\" if row['mean'] >= 3 else \"LOW\"\n",
    "    print(f\"   Cluster {cluster_id}: {row['mean']}/5 ({status} satisfaction) - {row['count']} feedback\")\n",
    "\n",
    "# Analyze feedback types distribution\n",
    "print(\"\\n2. FEEDBACK TYPE PATTERNS:\")\n",
    "feedback_patterns = df_clean.groupby(['kmeans_cluster', 'feedback_type']).size().unstack(fill_value=0)\n",
    "for cluster_id in feedback_patterns.index:\n",
    "    dominant_type = feedback_patterns.loc[cluster_id].idxmax()\n",
    "    print(f\"   Cluster {cluster_id}: Primarily {dominant_type}\")\n",
    "\n",
    "# Topic insights\n",
    "print(\"\\n3. TOPIC INSIGHTS:\")\n",
    "for topic_id, label in topic_labels.items():\n",
    "    topic_data = df_clean[df_clean['lda_topic'] == topic_id]\n",
    "    avg_rating = topic_data['rating'].mean()\n",
    "    sentiment = topic_data['sentiment_score'].mean()\n",
    "    \n",
    "    status = \"Positive\" if avg_rating >= 4 else \"Neutral\" if avg_rating >= 3 else \"Concerning\"\n",
    "    print(f\"   {label}: {avg_rating:.2f}/5 rating, {sentiment:.2f} sentiment ({status})\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ RECOMMENDATIONS:\")\n",
    "print(\"1. Focus improvement efforts on low-rating clusters\")\n",
    "print(\"2. Replicate success factors from high-rating clusters\") \n",
    "print(\"3. Address specific issues identified in topic analysis\")\n",
    "print(\"4. Monitor sentiment trends for early warning signs\")\n",
    "print(\"5. Use cluster insights for personalized member communication\")\n",
    "\n",
    "print(f\"\\nâœ… TOPIC CLUSTERING ANALYSIS COMPLETE!\")\n",
    "print(f\"ðŸš€ Ready for Phase 4: Visualization & Reporting\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
