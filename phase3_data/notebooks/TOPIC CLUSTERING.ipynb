{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a763d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TOPIC CLUSTERING ANALYSIS - GYM MANAGEMENT SYSTEM\n",
    "# Phase 3 - Task 9: Group user queries/feedback into thematic clusters\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SETUP AND IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "# Install required packages (run only once in Colab)\n",
    "!pip install wordcloud scikit-learn plotly\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Text processing and NLP\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Clustering algorithms\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Dimensionality reduction and visualization\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "\n",
    "# Word cloud and text visualization\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "# Metrics and evaluation\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')  # Added missing resource\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA LOADING AND INITIAL EXPLORATION\n",
    "# =============================================================================\n",
    "\n",
    "# Load the feedback data\n",
    "# Replace 'feedback_data.csv' with your actual file path\n",
    "df = pd.read_csv('feedback_data.csv')\n",
    "\n",
    "print(\"🔍 DATASET OVERVIEW\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n📊 FEEDBACK DISTRIBUTION\")\n",
    "print(\"=\"*50)\n",
    "print(\"Feedback types:\")\n",
    "print(df['feedback_type'].value_counts())\n",
    "print(\"\\nRating distribution:\")\n",
    "print(df['rating'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\n📝 TEXT DATA ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "df['text_length'] = df['feedback_text'].str.len()\n",
    "df['word_count'] = df['feedback_text'].str.split().str.len()\n",
    "\n",
    "print(f\"Average text length: {df['text_length'].mean():.1f} characters\")\n",
    "print(f\"Average word count: {df['word_count'].mean():.1f} words\")\n",
    "print(f\"Minimum words: {df['word_count'].min()}\")\n",
    "print(f\"Maximum words: {df['word_count'].max()}\")\n",
    "\n",
    "# Sample feedback texts\n",
    "print(\"\\n📋 SAMPLE FEEDBACK TEXTS\")\n",
    "print(\"=\"*50)\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i+1}: {df['feedback_text'].iloc[i]}\")\n",
    "    print(\"-\"*40)\n",
    "\n",
    "# =============================================================================\n",
    "# 3. TEXT PREPROCESSING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        # Add gym-specific stop words\n",
    "        self.stop_words.update(['gym', 'class', 'workout', 'exercise', 'session', 'coach', 'instructor'])\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    \n",
    "    def tokenize_and_lemmatize(self, text):\n",
    "        \"\"\"Tokenize and lemmatize text\"\"\"\n",
    "        try:\n",
    "            tokens = word_tokenize(text)\n",
    "        except LookupError:\n",
    "            # Fallback to simple split if NLTK resources are missing\n",
    "            tokens = text.split()\n",
    "        \n",
    "        # Remove stop words and lemmatize\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens \n",
    "                 if token not in self.stop_words and len(token) > 2]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "        text = self.clean_text(text)\n",
    "        text = self.tokenize_and_lemmatize(text)\n",
    "        return text\n",
    "\n",
    "# Initialize preprocessor and clean the data\n",
    "print(\"🧹 PREPROCESSING TEXT DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "df['cleaned_text'] = df['feedback_text'].apply(preprocessor.preprocess)\n",
    "\n",
    "# Show before and after examples\n",
    "print(\"BEFORE AND AFTER PREPROCESSING:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nOriginal: {df['feedback_text'].iloc[i]}\")\n",
    "    print(f\"Cleaned:  {df['cleaned_text'].iloc[i]}\")\n",
    "    print(\"-\"*40)\n",
    "\n",
    "# Remove empty or very short cleaned texts\n",
    "df_clean = df[df['cleaned_text'].str.len() > 10].copy()\n",
    "print(f\"\\n✅ Preprocessing complete! {len(df_clean)} valid feedback entries ready for clustering.\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. TEXT VECTORIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"🔤 TEXT VECTORIZATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=100,  # Top 100 features\n",
    "    min_df=2,          # Minimum document frequency\n",
    "    max_df=0.8,        # Maximum document frequency\n",
    "    ngram_range=(1, 2) # Unigrams and bigrams\n",
    ")\n",
    "\n",
    "# Fit and transform the cleaned text\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df_clean['cleaned_text'])\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {X_tfidf.shape}\")\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "\n",
    "# Show top features\n",
    "feature_scores = X_tfidf.sum(axis=0).A1\n",
    "top_features_idx = feature_scores.argsort()[-20:][::-1]\n",
    "print(\"\\nTop 20 features by TF-IDF score:\")\n",
    "for idx in top_features_idx:\n",
    "    print(f\"  {feature_names[idx]}: {feature_scores[idx]:.2f}\")\n",
    "\n",
    "# Count Vectorization (for LDA)\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_features=50,\n",
    "    min_df=2,\n",
    "    max_df=0.8,\n",
    "    ngram_range=(1, 1)  # Only unigrams for LDA\n",
    ")\n",
    "\n",
    "X_count = count_vectorizer.fit_transform(df_clean['cleaned_text'])\n",
    "count_feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"\\nCount matrix shape: {X_count.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 5. OPTIMAL NUMBER OF CLUSTERS ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"📊 DETERMINING OPTIMAL NUMBER OF CLUSTERS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Elbow method for K-means\n",
    "def find_optimal_clusters(X, max_k=10):\n",
    "    sse = []\n",
    "    silhouette_scores = []\n",
    "    calinski_scores = []\n",
    "    \n",
    "    K_range = range(2, max_k+1)\n",
    "    \n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X)\n",
    "        \n",
    "        sse.append(kmeans.inertia_)\n",
    "        \n",
    "        # Calculate silhouette score\n",
    "        sil_score = silhouette_score(X.toarray(), kmeans.labels_)\n",
    "        silhouette_scores.append(sil_score)\n",
    "        \n",
    "        # Calculate Calinski-Harabasz score\n",
    "        ch_score = calinski_harabasz_score(X.toarray(), kmeans.labels_)\n",
    "        calinski_scores.append(ch_score)\n",
    "    \n",
    "    return K_range, sse, silhouette_scores, calinski_scores\n",
    "\n",
    "K_range, sse, silhouette_scores, calinski_scores = find_optimal_clusters(X_tfidf)\n",
    "\n",
    "# Plot the results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Elbow curve\n",
    "axes[0].plot(K_range, sse, 'bo-')\n",
    "axes[0].set_xlabel('Number of Clusters (k)')\n",
    "axes[0].set_ylabel('Sum of Squared Errors (SSE)')\n",
    "axes[0].set_title('Elbow Method for Optimal k')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Silhouette scores\n",
    "axes[1].plot(K_range, silhouette_scores, 'ro-')\n",
    "axes[1].set_xlabel('Number of Clusters (k)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Analysis')\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Calinski-Harabasz scores\n",
    "axes[2].plot(K_range, calinski_scores, 'go-')\n",
    "axes[2].set_xlabel('Number of Clusters (k)')\n",
    "axes[2].set_ylabel('Calinski-Harabasz Score')\n",
    "axes[2].set_title('Calinski-Harabasz Analysis')\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal k based on silhouette score\n",
    "optimal_k = K_range[np.argmax(silhouette_scores)]\n",
    "print(f\"\\n🎯 Recommended number of clusters based on silhouette score: {optimal_k}\")\n",
    "print(f\"Best silhouette score: {max(silhouette_scores):.3f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 6. CLUSTERING ALGORITHMS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"🔍 APPLYING CLUSTERING ALGORITHMS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 6.1 K-Means Clustering\n",
    "print(\"1. K-Means Clustering\")\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "df_clean['kmeans_cluster'] = kmeans.fit_predict(X_tfidf)\n",
    "\n",
    "print(f\"K-Means cluster distribution:\")\n",
    "print(df_clean['kmeans_cluster'].value_counts().sort_index())\n",
    "\n",
    "# 6.2 DBSCAN Clustering\n",
    "print(\"\\n2. DBSCAN Clustering\")\n",
    "# Try different epsilon values\n",
    "eps_values = [0.3, 0.5, 0.7, 0.9]\n",
    "best_eps = 0.5\n",
    "best_n_clusters = 0\n",
    "\n",
    "for eps in eps_values:\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=3)\n",
    "    labels = dbscan.fit_predict(X_tfidf.toarray())\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    if n_clusters > best_n_clusters and n_clusters < 8:\n",
    "        best_eps = eps\n",
    "        best_n_clusters = n_clusters\n",
    "\n",
    "dbscan = DBSCAN(eps=best_eps, min_samples=3)\n",
    "df_clean['dbscan_cluster'] = dbscan.fit_predict(X_tfidf.toarray())\n",
    "\n",
    "print(f\"DBSCAN cluster distribution (eps={best_eps}):\")\n",
    "print(df_clean['dbscan_cluster'].value_counts().sort_index())\n",
    "\n",
    "# 6.3 Hierarchical Clustering\n",
    "print(\"\\n3. Hierarchical Clustering\")\n",
    "hierarchical = AgglomerativeClustering(n_clusters=optimal_k)\n",
    "df_clean['hierarchical_cluster'] = hierarchical.fit_predict(X_tfidf.toarray())\n",
    "\n",
    "print(f\"Hierarchical cluster distribution:\")\n",
    "print(df_clean['hierarchical_cluster'].value_counts().sort_index())\n",
    "\n",
    "# 6.4 Topic Modeling with LDA\n",
    "print(\"\\n4. Latent Dirichlet Allocation (LDA)\")\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=optimal_k,\n",
    "    random_state=42,\n",
    "    max_iter=100\n",
    ")\n",
    "lda.fit(X_count)\n",
    "\n",
    "# Get topic assignments\n",
    "topic_probs = lda.transform(X_count)\n",
    "df_clean['lda_topic'] = topic_probs.argmax(axis=1)\n",
    "\n",
    "print(f\"LDA topic distribution:\")\n",
    "print(df_clean['lda_topic'].value_counts().sort_index())\n",
    "\n",
    "# =============================================================================\n",
    "# 7. CLUSTER ANALYSIS AND INTERPRETATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"🔬 CLUSTER ANALYSIS AND INTERPRETATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def analyze_clusters(df, cluster_col, vectorizer, X, method_name):\n",
    "    \"\"\"Analyze and interpret clusters\"\"\"\n",
    "    print(f\"\\n{method_name.upper()} CLUSTER ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    for cluster_id in sorted(df[cluster_col].unique()):\n",
    "        if cluster_id == -1:  # Skip noise points in DBSCAN\n",
    "            continue\n",
    "            \n",
    "        cluster_mask = df[cluster_col] == cluster_id\n",
    "        cluster_docs = df[cluster_mask]\n",
    "        \n",
    "        print(f\"\\n📁 CLUSTER {cluster_id} ({len(cluster_docs)} documents)\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Average rating in this cluster\n",
    "        avg_rating = cluster_docs['rating'].mean()\n",
    "        print(f\"Average Rating: {avg_rating:.2f}\")\n",
    "        \n",
    "        # Feedback types in this cluster\n",
    "        feedback_types = cluster_docs['feedback_type'].value_counts()\n",
    "        print(f\"Feedback Types: {dict(feedback_types)}\")\n",
    "        \n",
    "        # Top terms for this cluster\n",
    "        cluster_indices = cluster_docs.index\n",
    "        if hasattr(X, 'toarray'):\n",
    "            cluster_tfidf = X[cluster_indices].toarray().mean(axis=0)\n",
    "        else:\n",
    "            cluster_tfidf = X[cluster_indices].mean(axis=0)\n",
    "        \n",
    "        top_indices = cluster_tfidf.argsort()[-10:][::-1]\n",
    "        top_terms = [feature_names[i] for i in top_indices]\n",
    "        top_scores = [cluster_tfidf[i] for i in top_indices]\n",
    "        \n",
    "        print(\"Top Terms:\")\n",
    "        for term, score in zip(top_terms, top_scores):\n",
    "            print(f\"  • {term}: {score:.3f}\")\n",
    "        \n",
    "        # Sample feedback from this cluster\n",
    "        print(\"\\nSample Feedback:\")\n",
    "        sample_feedback = cluster_docs['feedback_text'].head(2)\n",
    "        for i, feedback in enumerate(sample_feedback, 1):\n",
    "            print(f\"  {i}. {feedback[:100]}...\")\n",
    "\n",
    "# Analyze K-Means clusters\n",
    "analyze_clusters(df_clean, 'kmeans_cluster', tfidf_vectorizer, X_tfidf, \"K-Means\")\n",
    "\n",
    "# =============================================================================\n",
    "# 8. LDA TOPIC INTERPRETATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n🎯 LDA TOPIC INTERPRETATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words=10):\n",
    "    \"\"\"Display topics from LDA model\"\"\"\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words_idx = topic.argsort()[-no_top_words:][::-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        top_weights = [topic[i] for i in top_words_idx]\n",
    "        \n",
    "        print(f\"\\n📋 TOPIC {topic_idx}\")\n",
    "        print(\"-\" * 30)\n",
    "        for word, weight in zip(top_words, top_weights):\n",
    "            print(f\"  {word}: {weight:.3f}\")\n",
    "        \n",
    "        topics.append({\n",
    "            'topic_id': topic_idx,\n",
    "            'words': top_words,\n",
    "            'weights': top_weights\n",
    "        })\n",
    "    \n",
    "    return topics\n",
    "\n",
    "topics = display_topics(lda, count_feature_names, no_top_words=8)\n",
    "\n",
    "# Interpret topics based on top words\n",
    "topic_labels = {}\n",
    "print(\"\\n🏷️ TOPIC LABELS (Interpretations)\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "for i, topic in enumerate(topics):\n",
    "    words = topic['words'][:5]  # Top 5 words\n",
    "    \n",
    "    # Simple heuristic labeling based on common words\n",
    "    if any(word in ['equipment', 'machine', 'facility'] for word in words):\n",
    "        label = \"Equipment/Facility Issues\"\n",
    "    elif any(word in ['coach', 'instructor', 'teaching'] for word in words):\n",
    "        label = \"Coach/Instruction Quality\"\n",
    "    elif any(word in ['challenging', 'hard', 'intense', 'difficult'] for word in words):\n",
    "        label = \"Workout Intensity\"\n",
    "    elif any(word in ['music', 'energy', 'fun', 'amazing'] for word in words):\n",
    "        label = \"Class Atmosphere\"\n",
    "    elif any(word in ['beginner', 'advanced', 'modification'] for word in words):\n",
    "        label = \"Difficulty Level\"\n",
    "    else:\n",
    "        label = f\"General Feedback {i+1}\"\n",
    "    \n",
    "    topic_labels[i] = label\n",
    "    print(f\"Topic {i}: {label}\")\n",
    "\n",
    "df_clean['topic_label'] = df_clean['lda_topic'].map(topic_labels)\n",
    "\n",
    "# =============================================================================\n",
    "# 9. VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"📊 CREATING VISUALIZATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 9.1 Cluster Distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# K-Means distribution\n",
    "df_clean['kmeans_cluster'].value_counts().sort_index().plot(kind='bar', ax=axes[0,0])\n",
    "axes[0,0].set_title('K-Means Cluster Distribution')\n",
    "axes[0,0].set_xlabel('Cluster')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "\n",
    "# DBSCAN distribution\n",
    "df_clean['dbscan_cluster'].value_counts().sort_index().plot(kind='bar', ax=axes[0,1])\n",
    "axes[0,1].set_title('DBSCAN Cluster Distribution')\n",
    "axes[0,1].set_xlabel('Cluster')\n",
    "axes[0,1].set_ylabel('Count')\n",
    "\n",
    "# LDA topic distribution\n",
    "df_clean['lda_topic'].value_counts().sort_index().plot(kind='bar', ax=axes[1,0])\n",
    "axes[1,0].set_title('LDA Topic Distribution')\n",
    "axes[1,0].set_xlabel('Topic')\n",
    "axes[1,0].set_ylabel('Count')\n",
    "\n",
    "# Rating distribution by cluster\n",
    "df_clean.groupby('kmeans_cluster')['rating'].mean().plot(kind='bar', ax=axes[1,1])\n",
    "axes[1,1].set_title('Average Rating by K-Means Cluster')\n",
    "axes[1,1].set_xlabel('Cluster')\n",
    "axes[1,1].set_ylabel('Average Rating')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 9.2 Word Clouds for each cluster\n",
    "print(\"\\n☁️ Generating Word Clouds\")\n",
    "\n",
    "def create_wordcloud(text_list, title):\n",
    "    \"\"\"Create word cloud from list of texts\"\"\"\n",
    "    text = ' '.join(text_list)\n",
    "    if len(text.strip()) > 0:\n",
    "        wordcloud = WordCloud(\n",
    "            width=800, \n",
    "            height=400, \n",
    "            background_color='white',\n",
    "            colormap='viridis'\n",
    "        ).generate(text)\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(title, fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create word clouds for top K-means clusters\n",
    "top_clusters = df_clean['kmeans_cluster'].value_counts().head(3).index\n",
    "\n",
    "for cluster_id in top_clusters:\n",
    "    cluster_texts = df_clean[df_clean['kmeans_cluster'] == cluster_id]['cleaned_text'].tolist()\n",
    "    create_wordcloud(cluster_texts, f'Word Cloud - K-Means Cluster {cluster_id}')\n",
    "\n",
    "# 9.3 2D Visualization of Clusters using t-SNE\n",
    "print(\"\\n🗺️ Creating 2D Cluster Visualization\")\n",
    "\n",
    "# Reduce dimensions for visualization\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(df_clean)-1))\n",
    "X_tsne = tsne.fit_transform(X_tfidf.toarray())\n",
    "\n",
    "# Create interactive plot with Plotly\n",
    "fig = px.scatter(\n",
    "    x=X_tsne[:, 0], \n",
    "    y=X_tsne[:, 1],\n",
    "    color=df_clean['kmeans_cluster'].astype(str),\n",
    "    title='t-SNE Visualization of Feedback Clusters',\n",
    "    labels={'x': 't-SNE 1', 'y': 't-SNE 2', 'color': 'Cluster'},\n",
    "    hover_data=[df_clean['rating'], df_clean['feedback_type']]\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# 9.4 Topic Distribution Heatmap\n",
    "print(\"\\n🔥 Creating Topic-Feedback Type Heatmap\")\n",
    "\n",
    "# Create crosstab for heatmap\n",
    "crosstab = pd.crosstab(df_clean['feedback_type'], df_clean['lda_topic'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(crosstab, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Feedback Type vs LDA Topic Distribution')\n",
    "plt.xlabel('LDA Topic')\n",
    "plt.ylabel('Feedback Type')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 10. RESULTS SUMMARY AND EXPORT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"📋 GENERATING RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create comprehensive results summary\n",
    "results_summary = {\n",
    "    'total_feedback': len(df_clean),\n",
    "    'optimal_clusters': optimal_k,\n",
    "    'clustering_methods': ['K-Means', 'DBSCAN', 'Hierarchical', 'LDA'],\n",
    "    'best_silhouette_score': max(silhouette_scores),\n",
    "    'topic_labels': topic_labels\n",
    "}\n",
    "\n",
    "print(\"🎯 CLUSTERING RESULTS SUMMARY\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Total feedback analyzed: {results_summary['total_feedback']}\")\n",
    "print(f\"Optimal number of clusters: {results_summary['optimal_clusters']}\")\n",
    "print(f\"Best silhouette score: {results_summary['best_silhouette_score']:.3f}\")\n",
    "\n",
    "print(f\"\\n📊 CLUSTER DISTRIBUTION:\")\n",
    "for method in ['kmeans_cluster', 'dbscan_cluster', 'lda_topic']:\n",
    "    print(f\"\\n{method.upper()}:\")\n",
    "    distribution = df_clean[method].value_counts().sort_index()\n",
    "    for cluster, count in distribution.items():\n",
    "        print(f\"  Cluster {cluster}: {count} feedback entries\")\n",
    "\n",
    "print(f\"\\n🏷️ IDENTIFIED TOPICS:\")\n",
    "for topic_id, label in topic_labels.items():\n",
    "    count = (df_clean['lda_topic'] == topic_id).sum()\n",
    "    avg_rating = df_clean[df_clean['lda_topic'] == topic_id]['rating'].mean()\n",
    "    print(f\"  Topic {topic_id} - {label}: {count} entries (avg rating: {avg_rating:.2f})\")\n",
    "\n",
    "# Export results to CSV\n",
    "export_df = df_clean[[\n",
    "    'member_id', 'feedback_type', 'rating', 'feedback_text', \n",
    "    'cleaned_text', 'kmeans_cluster', 'dbscan_cluster', \n",
    "    'lda_topic', 'topic_label', 'sentiment_score'\n",
    "]].copy()\n",
    "\n",
    "export_df.to_csv('feedback_clustering_results.csv', index=False)\n",
    "\n",
    "# Export cluster summaries\n",
    "cluster_summary = []\n",
    "for cluster_id in sorted(df_clean['kmeans_cluster'].unique()):\n",
    "    cluster_data = df_clean[df_clean['kmeans_cluster'] == cluster_id]\n",
    "    \n",
    "    # Get top terms\n",
    "    cluster_indices = cluster_data.index\n",
    "    cluster_tfidf = X_tfidf[cluster_indices].toarray().mean(axis=0)\n",
    "    top_indices = cluster_tfidf.argsort()[-5:][::-1]\n",
    "    top_terms = [feature_names[i] for i in top_indices]\n",
    "    \n",
    "    cluster_summary.append({\n",
    "        'cluster_id': cluster_id,\n",
    "        'size': len(cluster_data),\n",
    "        'avg_rating': cluster_data['rating'].mean(),\n",
    "        'top_terms': ', '.join(top_terms),\n",
    "        'dominant_feedback_type': cluster_data['feedback_type'].mode().iloc[0],\n",
    "        'sample_feedback': cluster_data['feedback_text'].iloc[0][:200] + \"...\"\n",
    "    })\n",
    "\n",
    "cluster_summary_df = pd.DataFrame(cluster_summary)\n",
    "cluster_summary_df.to_csv('cluster_summary.csv', index=False)\n",
    "\n",
    "print(f\"\\n✅ EXPORT COMPLETE!\")\n",
    "print(f\"📁 Files created:\")\n",
    "print(f\"  • feedback_clustering_results.csv - Detailed results with cluster assignments\")\n",
    "print(f\"  • cluster_summary.csv - Cluster summaries and characteristics\")\n",
    "\n",
    "# =============================================================================\n",
    "# 11. BUSINESS INSIGHTS AND RECOMMENDATIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n💡 BUSINESS INSIGHTS AND RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"🔍 KEY FINDINGS:\")\n",
    "\n",
    "# Analyze ratings by cluster\n",
    "print(\"\\n1. RATING ANALYSIS BY CLUSTER:\")\n",
    "rating_by_cluster = df_clean.groupby('kmeans_cluster')['rating'].agg(['mean', 'count']).round(2)\n",
    "for cluster_id, row in rating_by_cluster.iterrows():\n",
    "    status = \"HIGH\" if row['mean'] >= 4 else \"MEDIUM\" if row['mean'] >= 3 else \"LOW\"\n",
    "    print(f\"   Cluster {cluster_id}: {row['mean']}/5 ({status} satisfaction) - {row['count']} feedback\")\n",
    "\n",
    "# Analyze feedback types distribution\n",
    "print(\"\\n2. FEEDBACK TYPE PATTERNS:\")\n",
    "feedback_patterns = df_clean.groupby(['kmeans_cluster', 'feedback_type']).size().unstack(fill_value=0)\n",
    "for cluster_id in feedback_patterns.index:\n",
    "    dominant_type = feedback_patterns.loc[cluster_id].idxmax()\n",
    "    print(f\"   Cluster {cluster_id}: Primarily {dominant_type}\")\n",
    "\n",
    "# Topic insights\n",
    "print(\"\\n3. TOPIC INSIGHTS:\")\n",
    "for topic_id, label in topic_labels.items():\n",
    "    topic_data = df_clean[df_clean['lda_topic'] == topic_id]\n",
    "    avg_rating = topic_data['rating'].mean()\n",
    "    sentiment = topic_data['sentiment_score'].mean()\n",
    "    \n",
    "    status = \"Positive\" if avg_rating >= 4 else \"Neutral\" if avg_rating >= 3 else \"Concerning\"\n",
    "    print(f\"   {label}: {avg_rating:.2f}/5 rating, {sentiment:.2f} sentiment ({status})\")\n",
    "\n",
    "print(f\"\\n🎯 RECOMMENDATIONS:\")\n",
    "print(\"1. Focus improvement efforts on low-rating clusters\")\n",
    "print(\"2. Replicate success factors from high-rating clusters\") \n",
    "print(\"3. Address specific issues identified in topic analysis\")\n",
    "print(\"4. Monitor sentiment trends for early warning signs\")\n",
    "print(\"5. Use cluster insights for personalized member communication\")\n",
    "\n",
    "print(f\"\\n✅ TOPIC CLUSTERING ANALYSIS COMPLETE!\")\n",
    "print(f\"🚀 Ready for Phase 4: Visualization & Reporting\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
